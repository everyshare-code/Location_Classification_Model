{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e91bc2e5-03f6-4a02-b385-f5a8def8dafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytube opencv-python transformers\n",
    "#!pip install moviepy\n",
    "#!pip install lightning\n",
    "# !pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e89469a9-8ce9-4e09-8f04-a4fe4ba13642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "import os\n",
    "def download_video(url, path='./videos'):\n",
    "    yt = YouTube(url)\n",
    "    # 가장 높은 해상도의 스트림 선택\n",
    "    ys = yt.streams.get_highest_resolution()\n",
    "    # 영상 다운로드\n",
    "    ys.download(path)\n",
    "    print(f\"다운로드 완료: {ys.default_filename}\")\n",
    "    return os.path.join(path, ys.default_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f16ee63e-df39-4b1b-8c84-7d41a3d25775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다운로드 완료: 와 진짜 말도 안되는 미쳐버린 상상력으로 만들어낸 띵작 영화 [결말포함].mp4\n"
     ]
    }
   ],
   "source": [
    "youtube_url='https://www.youtube.com/watch?v=AkKgXCO6mBA&ab_channel=Popcorn%26CokeReview'\n",
    "video_path=download_video(youtube_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe3e118-577c-4c85-925d-6e020565f9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./videos/와 진짜 말도 안되는 미쳐버린 상상력으로 만들어낸 띵작 영화 [결말포함].mp4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f477ab46-6198-442a-9972-e83cf39aad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from transformers import ViTFeatureExtractor\n",
    "\n",
    "def extract_video_features(extractor, video_file, sample_every=1):\n",
    "    vc = cv2.VideoCapture(video_file)\n",
    "    frames = []\n",
    "    while vc.isOpened():\n",
    "        success, frame = vc.read()\n",
    "        if not success:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    vc.release()\n",
    "\n",
    "    features = extractor(images=frames, return_tensors=\"pt\")\n",
    "    return features[\"pixel_values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4342e2df-7c14-41cb-8935-a80f4753aeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "class VideoFramesDataset(Dataset):\n",
    "    def __init__(self, video_path, transform=None):\n",
    "        self.video_path = video_path\n",
    "        self.transform = transform\n",
    "        self.frames = self._load_frames()\n",
    "\n",
    "    def _load_frames(self):\n",
    "        cap = cv2.VideoCapture(self.video_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        return frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame = self.frames[idx]\n",
    "        if self.transform:\n",
    "            frame = self.transform(frame)\n",
    "        return frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4b37d94-a8a1-409c-b298-2eb96095a9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import ViTModel\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "# from torchmetrics import F1\n",
    "from transformers import ViTModel\n",
    "\n",
    "\n",
    "class SummaryModel(LightningModule):\n",
    "    def __init__(self, hidden_dim=768, individual_logs=None):\n",
    "        super().__init__()\n",
    "        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "        self.scorer = nn.Linear(hidden_dim, 1)\n",
    "        # self.sigmoid = nn.Sigmoid()\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        # self.train_f1 = F1()\n",
    "        # self.val_f1 = F1()\n",
    "        # self.test_f1 = F1()\n",
    "        self.individual_logs = individual_logs\n",
    "        self.tta_logs = defaultdict(list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vit(x).pooler_output\n",
    "        x = self.scorer(x)\n",
    "        # x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def run_batch(self, batch, batch_idx, metric, training=False):\n",
    "        video_name, image_features, labels = batch\n",
    "        video_name = video_name[0]\n",
    "        image_features = image_features.squeeze(0)\n",
    "        labels = labels.squeeze(0)\n",
    "\n",
    "        # Score - aggregated labels.\n",
    "        score = torch.sum(labels, dim=0)\n",
    "        score = torch.min(\n",
    "            score,\n",
    "            torch.ones(\n",
    "                score.shape[0],\n",
    "            ).to(score.device),\n",
    "        )\n",
    "        out = self(image_features).squeeze(1)\n",
    "        try:\n",
    "            loss = self.loss(out.double(), score)\n",
    "            preds = (torch.sigmoid(out) > 0.7).int()\n",
    "            metric.update(preds, score.int())\n",
    "            f1 = metric.compute()\n",
    "            tp, fp, tn, fn = metric._get_final_stats()\n",
    "            self.tta_logs[video_name].append((tp.item(), fp.item(), fn.item()))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            loss = 0\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.run_batch(batch, batch_idx, self.train_f1, training=True)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        self.log(\"train_f1\", self.train_f1.compute())\n",
    "        self.train_f1.reset()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self.run_batch(batch, batch_idx, self.val_f1)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        self.log(\"val_f1\", self.val_f1.compute())\n",
    "        self.val_f1.reset()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.run_batch(batch, batch_idx, self.test_f1)\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        f1 = self.test_f1.compute()\n",
    "        self.log(\"test_f1\", f1)\n",
    "        tp, fp, tn, fn = self.test_f1._get_final_stats()\n",
    "        print(f\"\\nTest f1: {f1}, TP: {tp}, FP: {fp}, TN: {tn}, fn: {fn}\")\n",
    "        self.test_f1.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
    "        return optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3eff1dd-0558-44e9-9db5-3e093534b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(ckpt_path, device='cpu'):\n",
    "    model = SummaryModel.load_from_checkpoint(ckpt_path).to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8046e1b-a76d-4f91-b33d-a5daabb01c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"IMAGEIO_FFMPEG_EXE\"] = \"/opt/homebrew/bin/ffmpeg\"\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "from transformers import ViTFeatureExtractor\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "import cv2\n",
    "def summarize_video(video_path, model, extractor, threshold=0.7):\n",
    "    transform = Compose([\n",
    "        Resize((224, 224)),  # ViT 입력 크기에 맞춰 조정\n",
    "        ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    dataset = VideoFramesDataset(video_path, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    preds = []\n",
    "    for batch in dataloader:\n",
    "        features = extractor(images=batch, return_tensors=\"pt\")[\"pixel_values\"].to('cuda' or 'cpu')\n",
    "        with torch.no_grad():\n",
    "            out = model(features).squeeze(1)\n",
    "            pred = (torch.sigmoid(out) > threshold).nonzero(as_tuple=True)[0]\n",
    "            preds.extend(pred + len(preds) * dataloader.batch_size)\n",
    "\n",
    "    # 요약 비디오 생성\n",
    "    clip = VideoFileClip(video_path)\n",
    "    clips = [clip.subclip(max(frame.item() / clip.fps - 1, 0), min(frame.item() / clip.fps + 1, clip.duration)) for frame in preds]\n",
    "    final_clip = concatenate_videoclips(clips)\n",
    "    summary_path = \"summary_video.mp4\"\n",
    "    final_clip.write_videofile(summary_path, codec=\"libx264\", audio_codec=\"aac\")\n",
    "    print(f'Summary video saved to {summary_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d34b943-cd3c-4d52-86fa-30cb1a1e9c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.4.9 to v2.2.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint model/summary.ckpt`\n"
     ]
    }
   ],
   "source": [
    "model_path = 'model/summary.ckpt'\n",
    "\n",
    "# 영상 다운로드\n",
    "# downloaded_video_path = download_video(video_path)\n",
    "\n",
    "# 모델 및 특징 추출기 로딩\n",
    "device = 'cpu' # 또는 'cuda' if GPU 사용\n",
    "model = load_model(model_path, device)\n",
    "extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# 영상 요약 및 저장\n",
    "summarize_video(video_path, model, extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f011f88a-d899-440f-8fe4-febd6a03757d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
