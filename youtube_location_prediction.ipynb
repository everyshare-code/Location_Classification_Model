{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aaa0ccf-ce71-474b-a03d-1ba3e77555c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python-headless scipy\n",
    "# !pip install ImageHash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6671989f-cbd5-4cc0-958a-5b9c700c3cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import os\n",
    "# import numpy as np\n",
    "\n",
    "# def extract_frames(video_path, output_dir, fps=2):\n",
    "#     \"\"\"\n",
    "#     비디오에서 초당 프레임을 추출합니다.\n",
    "#     \"\"\"\n",
    "#     frames=[]\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#     fps_video = cap.get(cv2.CAP_PROP_FPS)\n",
    "#     frame_rate_ratio = int(fps_video / fps)\n",
    "\n",
    "#     for i in range(length):\n",
    "#         ret, frame = cap.read()\n",
    "#         if not ret:\n",
    "#             break\n",
    "#         if i % frame_rate_ratio == 0:\n",
    "#             frames.append(frame)\n",
    "#             frame_path = os.path.join(output_dir, f\"frame_{i}.jpg\")\n",
    "#             cv2.imwrite(frame_path, frame)\n",
    "#     cap.release()\n",
    "#     return frames\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "131e8717-b69f-4a5e-9c05-af5429838827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import timedelta\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a7a3e0e-ed9d-43ab-a6dd-88466b690dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_batch_sharpness(batch_frames):\n",
    "    \"\"\"\n",
    "    배치 프레임의 선명도를 계산합니다.\n",
    "    \"\"\"\n",
    "    sharpness_values = []\n",
    "    for frame in batch_frames:\n",
    "        gray_image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        sharpness = cv2.Laplacian(gray_image, cv2.CV_64F).var()\n",
    "        sharpness_values.append(sharpness)\n",
    "    return sharpness_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6b9ee87-6357-40c3-93e2-36c98330f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_sharpest_frames(frames, timestamps, batch_size=4):\n",
    "    \"\"\"\n",
    "    주어진 배치에서 가장 선명한 프레임을 선택합니다.\n",
    "    \"\"\"\n",
    "    selected_frames = []\n",
    "    selected_timestamps = []\n",
    "    for i in range(0, len(frames), batch_size):\n",
    "        batch_frames = frames[i:i+batch_size]\n",
    "        batch_timestamps = timestamps[i:i+batch_size]\n",
    "        sharpness_values = calculate_batch_sharpness(batch_frames)\n",
    "        sharpest_index = np.argmax(sharpness_values)\n",
    "        selected_frames.append(batch_frames[sharpest_index])\n",
    "        selected_timestamps.append(batch_timestamps[sharpest_index])\n",
    "    return selected_frames, selected_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8dab0cc-ce78-4a53-9c85-29f682bee0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(frames, processor, model):\n",
    "    \"\"\"\n",
    "    프레임들로부터 특성을 추출합니다.\n",
    "    \"\"\"\n",
    "    inputs = processor(images=[Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)) for frame in frames], return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    features = outputs.last_hidden_state[:,0,:].detach().numpy()\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb18d1de-c704-4189-acd7-fdb15399022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_frames(features):\n",
    "    \"\"\"\n",
    "    프레임의 특성을 기반으로 그룹화합니다.\n",
    "    \"\"\"\n",
    "    similarity_matrix = cosine_similarity(features)\n",
    "    clustering = AgglomerativeClustering(n_clusters=None, metric='precomputed', linkage='average', distance_threshold=0.5)\n",
    "    clustering.fit(1 - similarity_matrix)\n",
    "    return clustering.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7306656-2dd7-4943-90d9-ae7ee1c3225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path, output_dir, fps=1, batch_size=4):\n",
    "    \"\"\"\n",
    "    비디오에서 프레임을 추출하여 선명도를 기준으로 선택, 그 후 특성 추출 및 그룹화를 통해 저장합니다.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    processor, model = load_model_and_processor()\n",
    "    frames, timestamps = extract_frames(video_path, fps)\n",
    "    selected_frames, selected_timestamps = select_sharpest_frames(frames, timestamps, batch_size)\n",
    "    features = extract_features(selected_frames, processor, model)\n",
    "    labels = cluster_frames(features)\n",
    "    save_frames(selected_frames, selected_timestamps, labels, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9650f05-318c-4378-b594-7a87d9b6a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, fps):\n",
    "    \"\"\"\n",
    "    비디오에서 프레임과 타임스탬프를 추출합니다.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps_video = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_rate_ratio = max(int(fps_video / fps), 1)\n",
    "\n",
    "    frames = []\n",
    "    timestamps = []\n",
    "    frame_count = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_count % frame_rate_ratio == 0:\n",
    "            frames.append(frame)\n",
    "            timestamps.append(frame_count / fps_video)\n",
    "        frame_count += 1\n",
    "    cap.release()\n",
    "    return frames, timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7767888-8b2b-4b9b-8e6e-cedfbcf99bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames(frames, timestamps, labels, output_dir):\n",
    "    \"\"\"\n",
    "    선택된 프레임을 저장하고 정보를 JSON 파일로 기록합니다.\n",
    "    \"\"\"\n",
    "    frame_info = []\n",
    "    for i, (frame, timestamp, label) in enumerate(zip(frames, timestamps, labels)):\n",
    "        file_name = f\"frame_{i}_group_{label}.jpg\"\n",
    "        frame_path = os.path.join(output_dir, file_name)\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        frame_info.append({\n",
    "            \"file_name\": file_name,\n",
    "            \"timestamp\": str(timedelta(seconds=float(timestamp))),\n",
    "            \"group\": int(label)  # Ensure label is in Python int type\n",
    "        })\n",
    "\n",
    "    json_path = os.path.join(output_dir, \"frame_info.json\")\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(frame_info, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbc6e9e4-9bb4-409f-9f05-c014956aa3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_processor():\n",
    "    \"\"\"\n",
    "    ViT 모델과 프로세서를 로드합니다.\n",
    "    \"\"\"\n",
    "    processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "    model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "    return processor, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4860993-0446-4564-94ee-c6ab6b8d7da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path='videos/와 진짜 말도 안되는 미쳐버린 상상력으로 만들어낸 띵작 영화 [결말포함].mp4'\n",
    "output_dir = 'frames'  # 출력 디렉토리 경로\n",
    "FPS=1\n",
    "BATCH_SIZE = 4  # 배치 크기\n",
    "process_video(video_path, output_dir, FPS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "713fbcb3-a509-4dc9-97a3-c3515c9fc701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeoCLIP(\n",
       "  (image_encoder): ImageEncoder(\n",
       "    (CLIP): CLIPModel(\n",
       "      (text_model): CLIPTextTransformer(\n",
       "        (embeddings): CLIPTextEmbeddings(\n",
       "          (token_embedding): Embedding(49408, 768)\n",
       "          (position_embedding): Embedding(77, 768)\n",
       "        )\n",
       "        (encoder): CLIPEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (vision_model): CLIPVisionTransformer(\n",
       "        (embeddings): CLIPVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "          (position_embedding): Embedding(257, 1024)\n",
       "        )\n",
       "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder): CLIPEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x CLIPEncoderLayer(\n",
       "              (self_attn): CLIPAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): CLIPMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
       "      (text_projection): Linear(in_features=768, out_features=768, bias=False)\n",
       "    )\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=768, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (location_encoder): LocationEncoder(\n",
       "    (LocEnc0): LocationEncoderCapsule(\n",
       "      (capsule): Sequential(\n",
       "        (0): GaussianEncoding()\n",
       "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (4): ReLU()\n",
       "        (5): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (6): ReLU()\n",
       "      )\n",
       "      (head): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (LocEnc1): LocationEncoderCapsule(\n",
       "      (capsule): Sequential(\n",
       "        (0): GaussianEncoding()\n",
       "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (4): ReLU()\n",
       "        (5): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (6): ReLU()\n",
       "      )\n",
       "      (head): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (LocEnc2): LocationEncoderCapsule(\n",
       "      (capsule): Sequential(\n",
       "        (0): GaussianEncoding()\n",
       "        (1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (4): ReLU()\n",
       "        (5): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (6): ReLU()\n",
       "      )\n",
       "      (head): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from geoclip import GeoCLIP\n",
    "\n",
    "model = GeoCLIP()\n",
    "model.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3d5be311-518e-4c02-9320-1f9ba0f05cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_info=None\n",
    "with open('frames/frame_info.json','r') as f:\n",
    "    frame_info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4de53db1-2a40-49a7-928e-94ad6d684e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_to_location(frame_info, model):\n",
    "    predict_locations = []\n",
    "    for frame in frame_info:\n",
    "        image_path = os.path.join('frames', frame['file_name'])\n",
    "        # 모델 예측. 실제 모델 API에 따라 조정이 필요할 수 있습니다.\n",
    "        top_pred_gps, top_pred_prob = model.predict(image_path, top_k=1)\n",
    "        prob = top_pred_prob[0].item()\n",
    "        if prob > 0.08:\n",
    "            # Tensor를 Python의 float으로 변환하고 소수점 여섯째 자리까지 포맷팅\n",
    "            lat = \"{:.6f}\".format(top_pred_gps[0][0].item())  # 첫 번째 요소의 latitude\n",
    "            lon = \"{:.6f}\".format(top_pred_gps[0][1].item())  # 첫 번째 요소의 longitude\n",
    "            prob = top_pred_prob[0].item()   # 확률 값 변환\n",
    "            \n",
    "            # 기존 프레임 정보에 위치 정보 추가\n",
    "            frame_with_location = {\n",
    "                **frame, \n",
    "                \"latitude\": float(lat), \n",
    "                \"longitude\": float(lon), \n",
    "                \"probability\": prob\n",
    "            }\n",
    "            predict_locations.append(frame_with_location)\n",
    "    \n",
    "    # JSON 파일로 저장\n",
    "    with open('predict_locations.json', 'w') as f:\n",
    "        json.dump(predict_locations, f, indent=4)\n",
    "    \n",
    "    return predict_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aff31644-b0d0-4c20-8ca3-1059c9d94c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_locations = frames_to_location(frame_info, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "45711abf-c537-4e8c-91db-61025db25318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predict_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3566fdcc-3467-4ccc-baf5-41a140d6e794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frame_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83d0ee4-848a-4741-b480-2e7c86154ec9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
